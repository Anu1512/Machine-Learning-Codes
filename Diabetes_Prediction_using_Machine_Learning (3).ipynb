{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "duquRB1Btj91",
        "outputId": "1fd57341-258c-460a-afa0-ffd024b92984"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 9 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   gender               100000 non-null  object \n",
            " 1   age                  100000 non-null  float64\n",
            " 2   hypertension         100000 non-null  int64  \n",
            " 3   heart_disease        100000 non-null  int64  \n",
            " 4   smoking_history      100000 non-null  object \n",
            " 5   bmi                  100000 non-null  float64\n",
            " 6   HbA1c_level          100000 non-null  float64\n",
            " 7   blood_glucose_level  100000 non-null  int64  \n",
            " 8   diabetes             100000 non-null  int64  \n",
            "dtypes: float64(3), int64(4), object(2)\n",
            "memory usage: 6.9+ MB\n",
            "None\n",
            "                 age  hypertension  heart_disease            bmi  \\\n",
            "count  100000.000000  100000.00000  100000.000000  100000.000000   \n",
            "mean       41.885856       0.07485       0.039420      27.320767   \n",
            "std        22.516840       0.26315       0.194593       6.636783   \n",
            "min         0.080000       0.00000       0.000000      10.010000   \n",
            "25%        24.000000       0.00000       0.000000      23.630000   \n",
            "50%        43.000000       0.00000       0.000000      27.320000   \n",
            "75%        60.000000       0.00000       0.000000      29.580000   \n",
            "max        80.000000       1.00000       1.000000      95.690000   \n",
            "\n",
            "         HbA1c_level  blood_glucose_level       diabetes  \n",
            "count  100000.000000        100000.000000  100000.000000  \n",
            "mean        5.527507           138.058060       0.085000  \n",
            "std         1.070672            40.708136       0.278883  \n",
            "min         3.500000            80.000000       0.000000  \n",
            "25%         4.800000           100.000000       0.000000  \n",
            "50%         5.800000           140.000000       0.000000  \n",
            "75%         6.200000           159.000000       0.000000  \n",
            "max         9.000000           300.000000       1.000000  \n",
            "0    91500\n",
            "1     8500\n",
            "Name: diabetes, dtype: int64\n",
            "Outlier Analysis:\n",
            "  1    95000\n",
            "-1     5000\n",
            "Name: outlier, dtype: int64\n",
            "LOF Analysis:\n",
            "  1    95000\n",
            "-1     5000\n",
            "Name: lof, dtype: int64\n",
            "Naive Bayes Accuracy: 96.09%\n",
            "SVM Accuracy: 96.37%\n",
            "Linear Regression Accuracy: 96.39%\n",
            "Random Forest Accuracy: 97.00%\n",
            "KNN Accuracy: 96.20%\n",
            "Decision Tree Accuracy: 95.74%\n",
            "[LightGBM] [Info] Number of positive: 4328, number of negative: 68112\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.005188 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 72440, number of used features: 4\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.059746 -> initscore=-2.756048\n",
            "[LightGBM] [Info] Start training from score -2.756048\n",
            "LightGBM Accuracy: 97.34%\n",
            "XGBoost Accuracy: 97.33%\n",
            "[LightGBM] [Info] Number of positive: 4328, number of negative: 68112\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.003242 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 72440, number of used features: 4\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.059746 -> initscore=-2.756048\n",
            "[LightGBM] [Info] Start training from score -2.756048\n",
            "\n",
            "Naive Bayes Accuracy: 96.09%\n",
            "\n",
            "Naive Bayes:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     17049\n",
            "           1       0.78      0.46      0.58      1062\n",
            "\n",
            "    accuracy                           0.96     18111\n",
            "   macro avg       0.87      0.73      0.78     18111\n",
            "weighted avg       0.96      0.96      0.96     18111\n",
            "\n",
            "\n",
            "SVM Accuracy: 96.37%\n",
            "\n",
            "SVM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98     17049\n",
            "           1       1.00      0.38      0.55      1062\n",
            "\n",
            "    accuracy                           0.96     18111\n",
            "   macro avg       0.98      0.69      0.77     18111\n",
            "weighted avg       0.97      0.96      0.96     18111\n",
            "\n",
            "\n",
            "Linear Regression Accuracy: 96.39%\n",
            "\n",
            "Linear Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     17049\n",
            "           1       0.85      0.47      0.60      1062\n",
            "\n",
            "    accuracy                           0.96     18111\n",
            "   macro avg       0.91      0.73      0.79     18111\n",
            "weighted avg       0.96      0.96      0.96     18111\n",
            "\n",
            "\n",
            "Random Forest Accuracy: 96.97%\n",
            "\n",
            "Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     17049\n",
            "           1       0.85      0.59      0.69      1062\n",
            "\n",
            "    accuracy                           0.97     18111\n",
            "   macro avg       0.91      0.79      0.84     18111\n",
            "weighted avg       0.97      0.97      0.97     18111\n",
            "\n",
            "\n",
            "KNN Accuracy: 96.20%\n",
            "\n",
            "KNN:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     17049\n",
            "           1       0.81      0.46      0.59      1062\n",
            "\n",
            "    accuracy                           0.96     18111\n",
            "   macro avg       0.89      0.73      0.78     18111\n",
            "weighted avg       0.96      0.96      0.96     18111\n",
            "\n",
            "\n",
            "Decision Tree Accuracy: 95.80%\n",
            "\n",
            "Decision Tree:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.98      0.98     17049\n",
            "           1       0.65      0.62      0.63      1062\n",
            "\n",
            "    accuracy                           0.96     18111\n",
            "   macro avg       0.81      0.80      0.81     18111\n",
            "weighted avg       0.96      0.96      0.96     18111\n",
            "\n",
            "[LightGBM] [Info] Number of positive: 4328, number of negative: 68112\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.007809 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 396\n",
            "[LightGBM] [Info] Number of data points in the train set: 72440, number of used features: 4\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.059746 -> initscore=-2.756048\n",
            "[LightGBM] [Info] Start training from score -2.756048\n",
            "\n",
            "LightGBM Accuracy: 97.34%\n",
            "\n",
            "LightGBM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     17049\n",
            "           1       0.97      0.56      0.71      1062\n",
            "\n",
            "    accuracy                           0.97     18111\n",
            "   macro avg       0.97      0.78      0.85     18111\n",
            "weighted avg       0.97      0.97      0.97     18111\n",
            "\n",
            "\n",
            "XGBoost Accuracy: 97.33%\n",
            "\n",
            "XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     17049\n",
            "           1       0.96      0.57      0.71      1062\n",
            "\n",
            "    accuracy                           0.97     18111\n",
            "   macro avg       0.97      0.78      0.85     18111\n",
            "weighted avg       0.97      0.97      0.97     18111\n",
            "\n",
            "\n",
            "Best-performing model: LightGBM with accuracy 97.34%\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "(async (port, path, width, height, cache, element) => {\n",
              "    if (!google.colab.kernel.accessAllowed && !cache) {\n",
              "      return;\n",
              "    }\n",
              "    element.appendChild(document.createTextNode(''));\n",
              "    const url = await google.colab.kernel.proxyPort(port, {cache});\n",
              "    const iframe = document.createElement('iframe');\n",
              "    iframe.src = new URL(path, url).toString();\n",
              "    iframe.height = height;\n",
              "    iframe.width = width;\n",
              "    iframe.style.border = 0;\n",
              "    iframe.allow = [\n",
              "        'accelerometer',\n",
              "        'autoplay',\n",
              "        'camera',\n",
              "        'clipboard-read',\n",
              "        'clipboard-write',\n",
              "        'gyroscope',\n",
              "        'magnetometer',\n",
              "        'microphone',\n",
              "        'serial',\n",
              "        'usb',\n",
              "        'xr-spatial-tracking',\n",
              "    ].join('; ');\n",
              "    element.appendChild(iframe);\n",
              "  })(8050, \"/\", \"100%\", 650, false, window.element)"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "import dash\n",
        "import dash_html_components as html\n",
        "from dash import dcc as dcc\n",
        "import dash_bootstrap_components as dbc\n",
        "import webbrowser\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from dash.dependencies import Input, Output, State\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv(\"/content/diabetes_prediction_dataset.csv\")\n",
        "\n",
        "# Check if the 'diabetes' column is present\n",
        "if 'diabetes' not in data.columns:\n",
        "    raise KeyError(\"Column 'diabetes' not found in the loaded dataset. Please check the column names in your CSV file.\")\n",
        "\n",
        "# Step 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "print(data['diabetes'].value_counts())\n",
        "\n",
        "# Step 3: Data Preprocessing\n",
        "# Step 3.2: Outlier Observation Analysis\n",
        "numeric_columns = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "\n",
        "# Select only the desired features\n",
        "selected_features = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']\n",
        "data = data[numeric_columns + ['diabetes']]\n",
        "\n",
        "outlier_detector = EllipticEnvelope(contamination=0.05)\n",
        "data['outlier'] = outlier_detector.fit_predict(data[numeric_columns])\n",
        "print(\"Outlier Analysis:\\n\", data['outlier'].value_counts())\n",
        "\n",
        "# Step 3.3: Local Outlier Factor (LOF)\n",
        "lof = LocalOutlierFactor(contamination=0.05)\n",
        "data['lof'] = lof.fit_predict(data[numeric_columns])\n",
        "print(\"LOF Analysis:\\n\", data['lof'].value_counts())\n",
        "\n",
        "# Remove outliers\n",
        "data = data[data['outlier'] != -1]\n",
        "data = data[data['lof'] != -1]\n",
        "\n",
        "# Drop temporary columns\n",
        "data = data.drop(['outlier', 'lof'], axis=1)\n",
        "\n",
        "if 'diabetes' not in data.columns:\n",
        "    raise KeyError(\"Column 'diabetes' not found after preprocessing. Please check the column names.\")\n",
        "\n",
        "# Step 5: One Hot Encoding\n",
        "categorical_columns = []  # No categorical columns in the selected features\n",
        "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "# Only select the desired features\n",
        "data = data[['age', 'bmi', 'HbA1c_level', 'blood_glucose_level', 'diabetes']]\n",
        "\n",
        "if 'diabetes' not in data.columns:\n",
        "    raise KeyError(\"Column 'diabetes' not found after one-hot encoding. Please check the column names.\")\n",
        "\n",
        "# Step 6: Base Models\n",
        "X = data.drop('diabetes', axis=1)\n",
        "y = data['diabetes']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "models = {\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'Linear Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'LightGBM': LGBMClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f'{model_name} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Step 7: Model Tuning\n",
        "# Hyperparameter tuning for each model\n",
        "# Model Tuning for Naive Bayes\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for SVM\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for Logistic Regression\n",
        "lr_classifier = LogisticRegression()\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for K Nearest Neighbor (KNN)\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for Decision Tree\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for LightGBM\n",
        "lgbm_classifier = LGBMClassifier()\n",
        "lgbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for XGBoost\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for Random Forest\n",
        "rf_params = {\n",
        "    'n_estimators': 100,       # Adjust the number of trees in the forest\n",
        "    'max_depth': None,         # Adjust the maximum depth of the tree\n",
        "    'min_samples_split': 2,    # Adjust the minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': 1      # Adjust the minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "rf_classifier = RandomForestClassifier(**rf_params, random_state=42)\n",
        "\n",
        "\n",
        "# Step 8: Comparison of Final Models\n",
        "\n",
        "best_accuracy = 0.0  # Variable to store the best accuracy\n",
        "best_model_name = \"\"  # Variable to store the name of the best-performing model\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "\n",
        "    # Calculate accuracy within the loop\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    # Print accuracy\n",
        "    print(f'\\n{model_name} Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "    # Print classification report\n",
        "    print(f'\\n{model_name}:\\n', classification_report(y_test, predictions))\n",
        "\n",
        "    # Check if the current model has a higher accuracy than the best one so far\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_model_name = model_name\n",
        "\n",
        "\n",
        "# Step 9: Reporting\n",
        "print(f\"\\nBest-performing model: {best_model_name} with accuracy {best_accuracy * 100:.2f}%\")\n",
        "\n",
        "# Save the best-performing model to a pickle file\n",
        "best_model = models[best_model_name]\n",
        "with open('best_model.pkl', 'wb') as model_file:\n",
        "    pickle.dump(best_model, model_file)\n",
        "\n",
        "# Save the StandardScaler to a pickle file\n",
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "with open('scaler.pkl', 'wb') as scaler_file:\n",
        "    pickle.dump(scaler, scaler_file)\n",
        "\n",
        "# Dash Web Application\n",
        "app = dash.Dash(external_stylesheets=[dbc.themes.BOOTSTRAP])\n",
        "\n",
        "def preprocess_input(age, bmi, hba1c, blood_glucose):\n",
        "    # Convert input values to DataFrame\n",
        "    input_data = pd.DataFrame({\n",
        "        'age': [age],\n",
        "        'bmi': [bmi],\n",
        "        'HbA1c_level': [hba1c],\n",
        "        'blood_glucose_level': [blood_glucose]\n",
        "    })\n",
        "\n",
        "    # Use the saved StandardScaler for preprocessing\n",
        "    with open('scaler.pkl', 'rb') as scaler_file:\n",
        "        loaded_scaler = pickle.load(scaler_file)\n",
        "\n",
        "    input_data_scaled = pd.DataFrame(loaded_scaler.transform(input_data), columns=input_data.columns)\n",
        "    return input_data_scaled\n",
        "\n",
        "def check_data(age, bmi, hba1c, blood_glucose):\n",
        "    # Load the best-performing model\n",
        "    with open('best_model.pkl', 'rb') as model_file:\n",
        "        model = pickle.load(model_file)\n",
        "\n",
        "    # Additional conditions for age, diabetic status, and pre-diabetic status\n",
        "    age_group = 'Adult' if age >= 18 else 'Child'\n",
        "    diabetic_status = 'Diabetic' if hba1c > 6.0 or blood_glucose > 140 else 'Non-Diabetic'\n",
        "    prediabetic_status = 'Pre-Diabetic' if 5.7 < hba1c <= 6.0 or 100 < blood_glucose <= 140 else 'Non-Diabetic'\n",
        "\n",
        "    # Preprocess input data\n",
        "    input_data_scaled = preprocess_input(age, bmi, hba1c, blood_glucose)\n",
        "\n",
        "    # Make predictions\n",
        "    prediction = model.predict(input_data_scaled)\n",
        "\n",
        "    return age_group, diabetic_status, prediabetic_status, prediction\n",
        "\n",
        "def UI_main():\n",
        "    main_ui = dbc.Container(html.Div(\n",
        "        [\n",
        "            dbc.Alert(children=\"Diabetes Prediction Checker\", id=\"Main_head\", color=\"success\", style={'fontSize': '24px'}),\n",
        "            dbc.Label(\"Age\"),\n",
        "            dbc.Input(id=\"age_input\", placeholder=\"Age of the patient\", type=\"number\", min=0),\n",
        "            html.Br(),\n",
        "            dbc.Label(\"BMI\"),\n",
        "            dbc.Input(id=\"BMI_input\", placeholder=\"BMI of the patient\", type=\"number\", min=0),\n",
        "            html.Br(),\n",
        "            dbc.Label(\"HbA1c Level\"),\n",
        "            dbc.Input(id=\"HbA1c_input\", placeholder=\"HbA1c Level of the patient\", type=\"number\", min=0),\n",
        "            html.Br(),\n",
        "            dbc.Label(\"Blood Glucose Level\"),\n",
        "            dbc.Input(id=\"BloodGlucose_input\", placeholder=\"Blood Glucose Level of the patient\", type=\"number\", min=0),\n",
        "            html.Br(),\n",
        "            dbc.Button(\"Submit\", id=\"button\", color='secondary', className=\"mt-3\", style={'width': '100%', 'fontSize': '24px'}),\n",
        "            dbc.Button(children=\"Result\", id='result', color='light', className=\"mt-3\", disabled=True,\n",
        "                       style={'width': '100%', 'fontSize': '20px'}),\n",
        "        ],\n",
        "        className=\"p-5\", style={'marginTop': '20px'}\n",
        "    ))\n",
        "    return main_ui\n",
        "\n",
        "@app.callback(\n",
        "    Output('result', 'children'),\n",
        "    Output('result', 'color'),\n",
        "    Output('result', 'disabled'),\n",
        "    [\n",
        "        Input('button', 'n_clicks'),\n",
        "        Input('age_input', 'value'),\n",
        "        Input('BMI_input', 'value'),\n",
        "        Input('HbA1c_input', 'value'),\n",
        "        Input('BloodGlucose_input', 'value')\n",
        "    ],\n",
        "    [\n",
        "        State('button', 'n_clicks_previous')\n",
        "    ]\n",
        ")\n",
        "def review_update(n_clicks, age, bmi, hba1c, blood_glucose, n_clicks_previous):\n",
        "    print(\"Callback function called.\")\n",
        "    print(f\"Inputs: n_clicks={n_clicks}, age={age}, bmi={bmi}, hba1c={hba1c}, blood_glucose={blood_glucose}, n_clicks_previous={n_clicks_previous}\")\n",
        "\n",
        "    if n_clicks is not None and (n_clicks_previous is None or n_clicks > n_clicks_previous):\n",
        "        age_group, diabetic_status, prediabetic_status, response = check_data(age, bmi, hba1c, blood_glucose)\n",
        "        print(\"Response:\", response)\n",
        "\n",
        "        if response and response[0] == 0:\n",
        "            return f'The patient is {age_group} and {diabetic_status} ({prediabetic_status})', 'success', False\n",
        "        elif response:\n",
        "            return f'The patient is {age_group} and {diabetic_status} ({prediabetic_status})', 'danger', False\n",
        "    return \"\", \"light\", True\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    app.title = \"Diabetes Prediction\"\n",
        "    app.layout = UI_main()\n",
        "    app.run_server()\n",
        "    webbrowser.open_new(\"http://127.0.0.1:8050/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PfH6vcuuN-2",
        "outputId": "40c6f72f-2523-416c-ebc8-3b125a6fed21"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting dash_bootstrap_components\n",
            "  Downloading dash_bootstrap_components-1.5.0-py3-none-any.whl (221 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m221.2/221.2 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dash>=2.0.0 (from dash_bootstrap_components)\n",
            "  Downloading dash-2.16.1-py3-none-any.whl (10.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.2/10.2 MB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (3.0.1)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (5.15.0)\n",
            "Collecting dash-html-components==2.0.0 (from dash>=2.0.0->dash_bootstrap_components)\n",
            "  Downloading dash_html_components-2.0.0-py3-none-any.whl (4.1 kB)\n",
            "Collecting dash-core-components==2.0.0 (from dash>=2.0.0->dash_bootstrap_components)\n",
            "  Downloading dash_core_components-2.0.0-py3-none-any.whl (3.8 kB)\n",
            "Collecting dash-table==5.0.0 (from dash>=2.0.0->dash_bootstrap_components)\n",
            "  Downloading dash_table-5.0.0-py3-none-any.whl (3.9 kB)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (7.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (2.31.0)\n",
            "Collecting retrying (from dash>=2.0.0->dash_bootstrap_components)\n",
            "  Downloading retrying-1.3.4-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash>=2.0.0->dash_bootstrap_components) (67.7.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash>=2.0.0->dash_bootstrap_components) (8.1.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.0.0->dash_bootstrap_components) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash>=2.0.0->dash_bootstrap_components) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<3.1->dash>=2.0.0->dash_bootstrap_components) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash>=2.0.0->dash_bootstrap_components) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.0.0->dash_bootstrap_components) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.0.0->dash_bootstrap_components) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.0.0->dash_bootstrap_components) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash>=2.0.0->dash_bootstrap_components) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash>=2.0.0->dash_bootstrap_components) (1.16.0)\n",
            "Installing collected packages: dash-table, dash-html-components, dash-core-components, retrying, dash, dash_bootstrap_components\n",
            "Successfully installed dash-2.16.1 dash-core-components-2.0.0 dash-html-components-2.0.0 dash-table-5.0.0 dash_bootstrap_components-1.5.0 retrying-1.3.4\n"
          ]
        }
      ],
      "source": [
        "pip install dash_bootstrap_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ma8GUSaruRa8",
        "outputId": "9780c210-448c-495f-dcf4-2999a922679b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dash_html_components in /usr/local/lib/python3.10/dist-packages (2.0.0)\n"
          ]
        }
      ],
      "source": [
        "pip install dash_html_components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWO9Rtq8t9XG",
        "outputId": "3d578c16-2483-4b8e-9fff-46988a43d5a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: dash in /usr/local/lib/python3.10/dist-packages (2.16.1)\n",
            "Requirement already satisfied: Flask<3.1,>=1.0.4 in /usr/local/lib/python3.10/dist-packages (from dash) (2.2.5)\n",
            "Requirement already satisfied: Werkzeug<3.1 in /usr/local/lib/python3.10/dist-packages (from dash) (3.0.1)\n",
            "Requirement already satisfied: plotly>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (5.15.0)\n",
            "Requirement already satisfied: dash-html-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (2.0.0)\n",
            "Requirement already satisfied: dash-core-components==2.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (2.0.0)\n",
            "Requirement already satisfied: dash-table==5.0.0 in /usr/local/lib/python3.10/dist-packages (from dash) (5.0.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.10/dist-packages (from dash) (7.0.1)\n",
            "Requirement already satisfied: typing-extensions>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from dash) (4.10.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from dash) (2.31.0)\n",
            "Requirement already satisfied: retrying in /usr/local/lib/python3.10/dist-packages (from dash) (1.3.4)\n",
            "Requirement already satisfied: nest-asyncio in /usr/local/lib/python3.10/dist-packages (from dash) (1.6.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from dash) (67.7.2)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask<3.1,>=1.0.4->dash) (8.1.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash) (8.2.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from plotly>=5.0.0->dash) (23.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from Werkzeug<3.1->dash) (2.1.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->dash) (3.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->dash) (2024.2.2)\n",
            "Requirement already satisfied: six>=1.7.0 in /usr/local/lib/python3.10/dist-packages (from retrying->dash) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "pip install dash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-leloBq5t_6R",
        "outputId": "1c27c16d-ade6-4e67-bb9f-e02f3e3ef02e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "pip install pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tlem0hPl7Zff",
        "outputId": "aa0c2db0-9e1b-4afc-b222-e0b088d67785"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (1.5.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.10/dist-packages (4.1.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.10/dist-packages (2.0.3)\n",
            "Requirement already satisfied: Flask in /usr/local/lib/python3.10/dist-packages (2.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2023.4)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from Flask) (8.1.7)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->Flask) (2.1.5)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas numpy scikit-learn lightgbm xgboost Flask\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyjFjjGG71Xv",
        "outputId": "b43a03f7-a6e7-4294-dd92-88c77f7957f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[31mERROR: Could not find a version that satisfies the requirement pickle (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for pickle\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MuJTO3C87oXk",
        "outputId": "4deead39-abeb-4983-de38-39feb32dd727"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Models saved as pickle files.\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pickle\n",
        "\n",
        "# Assuming 'models' is a dictionary containing your trained models\n",
        "models = {\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'Linear Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'LightGBM': LGBMClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}\n",
        "\n",
        "# Save each model to a pickle file\n",
        "for model_name, model in models.items():\n",
        "    with open(f'{model_name}_model.pkl', 'wb') as file:\n",
        "        pickle.dump(model, file)\n",
        "\n",
        "print(\"Models saved as pickle files.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxaPu06N361B"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.covariance import EllipticEnvelope\n",
        "from sklearn.ensemble import IsolationForest"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HBNUw8rq40ca"
      },
      "outputs": [],
      "source": [
        "# Step 1: Load the dataset\n",
        "data = pd.read_csv('/content/diabetes_prediction_dataset.csv')\n",
        "print(data)\n",
        "\n",
        "if 'diabetes' not in data.columns:\n",
        "    raise KeyError(\"Column 'diabetes' not found in the loaded dataset.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xRIwshFH_Nix",
        "outputId": "685c672f-904a-456f-b553-57eeaafa7223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 100000 entries, 0 to 99999\n",
            "Data columns (total 9 columns):\n",
            " #   Column               Non-Null Count   Dtype  \n",
            "---  ------               --------------   -----  \n",
            " 0   gender               100000 non-null  object \n",
            " 1   age                  100000 non-null  float64\n",
            " 2   hypertension         100000 non-null  int64  \n",
            " 3   heart_disease        100000 non-null  int64  \n",
            " 4   smoking_history      100000 non-null  object \n",
            " 5   bmi                  100000 non-null  float64\n",
            " 6   HbA1c_level          100000 non-null  float64\n",
            " 7   blood_glucose_level  100000 non-null  int64  \n",
            " 8   diabetes             100000 non-null  int64  \n",
            "dtypes: float64(3), int64(4), object(2)\n",
            "memory usage: 6.9+ MB\n",
            "None\n",
            "                 age  hypertension  heart_disease            bmi  \\\n",
            "count  100000.000000  100000.00000  100000.000000  100000.000000   \n",
            "mean       41.885856       0.07485       0.039420      27.320767   \n",
            "std        22.516840       0.26315       0.194593       6.636783   \n",
            "min         0.080000       0.00000       0.000000      10.010000   \n",
            "25%        24.000000       0.00000       0.000000      23.630000   \n",
            "50%        43.000000       0.00000       0.000000      27.320000   \n",
            "75%        60.000000       0.00000       0.000000      29.580000   \n",
            "max        80.000000       1.00000       1.000000      95.690000   \n",
            "\n",
            "         HbA1c_level  blood_glucose_level       diabetes  \n",
            "count  100000.000000        100000.000000  100000.000000  \n",
            "mean        5.527507           138.058060       0.085000  \n",
            "std         1.070672            40.708136       0.278883  \n",
            "min         3.500000            80.000000       0.000000  \n",
            "25%         4.800000           100.000000       0.000000  \n",
            "50%         5.800000           140.000000       0.000000  \n",
            "75%         6.200000           159.000000       0.000000  \n",
            "max         9.000000           300.000000       1.000000  \n",
            "0    91500\n",
            "1     8500\n",
            "Name: diabetes, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 2: Exploratory Data Analysis (EDA)\n",
        "\n",
        "print(data.info())\n",
        "print(data.describe())\n",
        "print(data['diabetes'].value_counts())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96GLOJyV_a6Z",
        "outputId": "576f570b-9fec-4a81-afe3-68ebc9de1d0a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Missing Values:\n",
            " gender                 0\n",
            "age                    0\n",
            "hypertension           0\n",
            "heart_disease          0\n",
            "smoking_history        0\n",
            "bmi                    0\n",
            "HbA1c_level            0\n",
            "blood_glucose_level    0\n",
            "diabetes               0\n",
            "dtype: int64\n",
            "Outlier Analysis:\n",
            "  1    95000\n",
            "-1     5000\n",
            "Name: outlier, dtype: int64\n",
            "LOF Analysis:\n",
            "  1    95000\n",
            "-1     5000\n",
            "Name: lof, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Step 3: Data Preprocessing\n",
        "\n",
        "# Step 3.1: Missing Observation Analysis\n",
        "\n",
        "missing_values = data.isnull().sum()\n",
        "print(\"Missing Values:\\n\", missing_values)\n",
        "\n",
        "# Step 3.2: Outlier Observation Analysis\n",
        "\n",
        "numeric_columns = ['age', 'bmi', 'HbA1c_level', 'blood_glucose_level']\n",
        "outlier_detector = EllipticEnvelope(contamination=0.05)\n",
        "data['outlier'] = outlier_detector.fit_predict(data[numeric_columns])\n",
        "print(\"Outlier Analysis:\\n\", data['outlier'].value_counts())\n",
        "\n",
        "# Step 3.3: Local Outlier Factor (LOF)\n",
        "\n",
        "from sklearn.neighbors import LocalOutlierFactor\n",
        "lof = LocalOutlierFactor(contamination=0.05)\n",
        "data['lof'] = lof.fit_predict(data[numeric_columns])\n",
        "print(\"LOF Analysis:\\n\", data['lof'].value_counts())\n",
        "\n",
        "# Remove outliers\n",
        "data = data[data['outlier'] != -1]\n",
        "data = data[data['lof'] != -1]\n",
        "\n",
        "# Drop temporary columns\n",
        "data = data.drop(['outlier', 'lof'], axis=1)\n",
        "\n",
        "if 'diabetes' not in data.columns:\n",
        "    raise KeyError(\"Column 'diabetes' not found after preprocessing.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rSxFqqXs_i_X"
      },
      "outputs": [],
      "source": [
        "# Step 4: One Hot Encoding\n",
        "categorical_columns = ['gender', 'hypertension', 'heart_disease', 'smoking_history']\n",
        "data = pd.get_dummies(data, columns=categorical_columns, drop_first=True)\n",
        "\n",
        "if 'diabetes' not in data.columns:\n",
        "    raise KeyError(\"Column 'diabetes' not found after one-hot encoding.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xUuXBXdg_xtm",
        "outputId": "088809df-e6ef-41b0-d17a-b819a693e17c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Naive Bayes Accuracy: 85.98%\n",
            "SVM Accuracy: 95.90%\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Linear Regression Accuracy: 96.08%\n",
            "Random Forest Accuracy: 97.11%\n",
            "KNN Accuracy: 96.21%\n",
            "Decision Tree Accuracy: 95.34%\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 3378, number of negative: 69190\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008724 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 409\n",
            "[LightGBM] [Info] Number of data points in the train set: 72568, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.046549 -> initscore=-3.019573\n",
            "[LightGBM] [Info] Start training from score -3.019573\n",
            "LightGBM Accuracy: 97.35%\n",
            "XGBoost Accuracy: 97.28%\n"
          ]
        }
      ],
      "source": [
        "# Step 5: Base Models\n",
        "X = data.drop('diabetes', axis=1)\n",
        "y = data['diabetes']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "models = {\n",
        "    'Naive Bayes': GaussianNB(),\n",
        "    'SVM': SVC(),\n",
        "    'Linear Regression': LogisticRegression(),\n",
        "    'Random Forest': RandomForestClassifier(),\n",
        "    'KNN': KNeighborsClassifier(),\n",
        "    'Decision Tree': DecisionTreeClassifier(),\n",
        "    'LightGBM': LGBMClassifier(),\n",
        "    'XGBoost': XGBClassifier()\n",
        "}\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "    print(f'{model_name} Accuracy: {accuracy * 100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "3e7hismY_z2y",
        "outputId": "c648ee28-1b10-4487-fa3e-d271eb665eeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 3378, number of negative: 69190\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008868 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 409\n",
            "[LightGBM] [Info] Number of data points in the train set: 72568, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.046549 -> initscore=-3.019573\n",
            "[LightGBM] [Info] Start training from score -3.019573\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(random_state=42)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(random_state=42)</pre></div></div></div></div></div>"
            ],
            "text/plain": [
              "RandomForestClassifier(random_state=42)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Step 6: Model Tuning\n",
        "\n",
        "# Hyperparameter tuning for each model\n",
        "# Model Tuning for Naive Bayes\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb_classifier = GaussianNB()\n",
        "nb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for SVM\n",
        "from sklearn.svm import SVC\n",
        "svm_classifier = SVC()\n",
        "svm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for Logistic Regression\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "lr_classifier = LogisticRegression()\n",
        "lr_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for K Nearest Neighbor (KNN)\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn_classifier = KNeighborsClassifier()\n",
        "knn_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for Decision Tree\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "dt_classifier = DecisionTreeClassifier()\n",
        "dt_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for LightGBM\n",
        "from lightgbm import LGBMClassifier\n",
        "lgbm_classifier = LGBMClassifier()\n",
        "lgbm_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for XGBoost\n",
        "from xgboost import XGBClassifier\n",
        "xgb_classifier = XGBClassifier()\n",
        "xgb_classifier.fit(X_train, y_train)\n",
        "\n",
        "# Model Tuning for Random Forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf_params = {\n",
        "    'n_estimators': 100,       # Adjusting the number of trees in the forest\n",
        "    'max_depth': None,         # Adjusting the maximum depth of the tree\n",
        "    'min_samples_split': 2,    # Adjusting the minimum number of samples required to split an internal node\n",
        "    'min_samples_leaf': 1      # Adjusting the minimum number of samples required to be at a leaf node\n",
        "}\n",
        "\n",
        "rf_classifier = RandomForestClassifier(**rf_params, random_state=42)\n",
        "rf_classifier.fit(X_train, y_train)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44IQEYtX_7-F",
        "outputId": "5c60bca9-b286-4f91-c0e1-5b901a3ef406"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Naive Bayes:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.87      0.92     17344\n",
            "           1       0.19      0.65      0.29       799\n",
            "\n",
            "    accuracy                           0.86     18143\n",
            "   macro avg       0.58      0.76      0.61     18143\n",
            "weighted avg       0.95      0.86      0.89     18143\n",
            "\n",
            "\n",
            "SVM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      1.00      0.98     17344\n",
            "           1       1.00      0.07      0.13       799\n",
            "\n",
            "    accuracy                           0.96     18143\n",
            "   macro avg       0.98      0.53      0.55     18143\n",
            "weighted avg       0.96      0.96      0.94     18143\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Linear Regression:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     17344\n",
            "           1       0.63      0.26      0.37       799\n",
            "\n",
            "    accuracy                           0.96     18143\n",
            "   macro avg       0.80      0.63      0.67     18143\n",
            "weighted avg       0.95      0.96      0.95     18143\n",
            "\n",
            "\n",
            "Random Forest:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     17344\n",
            "           1       0.83      0.43      0.57       799\n",
            "\n",
            "    accuracy                           0.97     18143\n",
            "   macro avg       0.90      0.72      0.78     18143\n",
            "weighted avg       0.97      0.97      0.97     18143\n",
            "\n",
            "\n",
            "KNN:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      0.99      0.98     17344\n",
            "           1       0.68      0.27      0.38       799\n",
            "\n",
            "    accuracy                           0.96     18143\n",
            "   macro avg       0.82      0.63      0.68     18143\n",
            "weighted avg       0.95      0.96      0.95     18143\n",
            "\n",
            "\n",
            "Decision Tree:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.97      0.97     17344\n",
            "           1       0.46      0.53      0.49       799\n",
            "\n",
            "    accuracy                           0.95     18143\n",
            "   macro avg       0.72      0.75      0.73     18143\n",
            "weighted avg       0.96      0.95      0.95     18143\n",
            "\n",
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 3378, number of negative: 69190\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.008509 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 409\n",
            "[LightGBM] [Info] Number of data points in the train set: 72568, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.046549 -> initscore=-3.019573\n",
            "[LightGBM] [Info] Start training from score -3.019573\n",
            "\n",
            "LightGBM:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     17344\n",
            "           1       0.92      0.44      0.59       799\n",
            "\n",
            "    accuracy                           0.97     18143\n",
            "   macro avg       0.95      0.72      0.79     18143\n",
            "weighted avg       0.97      0.97      0.97     18143\n",
            "\n",
            "\n",
            "XGBoost:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.97      1.00      0.99     17344\n",
            "           1       0.88      0.44      0.59       799\n",
            "\n",
            "    accuracy                           0.97     18143\n",
            "   macro avg       0.93      0.72      0.79     18143\n",
            "weighted avg       0.97      0.97      0.97     18143\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Step 7: Comparison of Final Models\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    print(f'\\n{model_name}:\\n', classification_report(y_test, predictions))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ml76sFhTAApF",
        "outputId": "a237a550-c42f-4ed1-b103-153dbc3ff2f2"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
            "[LightGBM] [Info] Number of positive: 3378, number of negative: 69190\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.005849 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 409\n",
            "[LightGBM] [Info] Number of data points in the train set: 72568, number of used features: 12\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.046549 -> initscore=-3.019573\n",
            "[LightGBM] [Info] Start training from score -3.019573\n",
            "\n",
            " Reporting\n",
            "In conclusion, we explored the dataset, handled missing values, identified and removed outliers, performed feature engineering, and applied one-hot encoding.\n",
            "We trained various machine learning models, including Naive Bayes, SVM, Linear Regression, Random Forest, KNN, Decision Tree, LightGBM, and XGBoost.\n",
            "After tuning hyperparameters and evaluating the models, we observed that LightGBM achieved the highest accuracy of 97.35%.\n",
            "This model can be considered for further deployment and usage in predicting diabetes based on the given features.\n"
          ]
        }
      ],
      "source": [
        "# Step 8: Reporting\n",
        "\n",
        "# To Find the best-performing model\n",
        "best_model_name = \"\"  # Variable to store the name of the best-performing model\n",
        "best_model_accuracy = 0\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    model.fit(X_train, y_train)\n",
        "    predictions = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, predictions)\n",
        "\n",
        "    # Update the best-performing model\n",
        "    if accuracy > best_model_accuracy:\n",
        "        best_model_accuracy = accuracy\n",
        "        best_model_name = model_name\n",
        "\n",
        "print(\"\\n Reporting\")\n",
        "print(f\"In conclusion, we explored the dataset, handled missing values, identified and removed outliers, performed feature engineering, and applied one-hot encoding.\")\n",
        "print(f\"We trained various machine learning models, including Naive Bayes, SVM, Linear Regression, Random Forest, KNN, Decision Tree, LightGBM, and XGBoost.\")\n",
        "\n",
        "if best_model_name:\n",
        "    print(f\"After tuning hyperparameters and evaluating the models, we observed that {best_model_name} achieved the highest accuracy of {best_model_accuracy * 100:.2f}%.\")\n",
        "    print(f\"This model can be considered for further deployment and usage in predicting diabetes based on the given features.\")\n",
        "else:\n",
        "    print(\"No best-performing model found. Please review the results of each model.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}